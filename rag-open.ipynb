{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "015e515d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load your Excel file\n",
    "df = pd.read_excel(\"test02.xlsx\")\n",
    "\n",
    "# Combine columns into a single document string for each row\n",
    "df[\"content\"] = (\n",
    "    df[\"Systeme\"].astype(str) + \" | \" +\n",
    "    df[\"Description\"].astype(str) + \" | \" +\n",
    "    df[\"Description de l'équipement\"].astype(str)\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ae490981",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 12\u001b[39m\n\u001b[32m      9\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m response.data[\u001b[32m0\u001b[39m].embedding\n\u001b[32m     11\u001b[39m \u001b[38;5;66;03m# Compute embeddings for all rows\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m12\u001b[39m df[\u001b[33m\"\u001b[39m\u001b[33membedding\u001b[39m\u001b[33m\"\u001b[39m] = \u001b[43mdf\u001b[49m[\u001b[33m\"\u001b[39m\u001b[33mcontent\u001b[39m\u001b[33m\"\u001b[39m].apply(\u001b[38;5;28;01mlambda\u001b[39;00m x: get_embedding(x))\n",
      "\u001b[31mNameError\u001b[39m: name 'df' is not defined"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from openai import OpenAI\n",
    "\n",
    "client = OpenAI(api_key=\"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\")\n",
    "\n",
    "def get_embedding(text, model=\"text-embedding-3-small\"):\n",
    "    response = client.embeddings.create(input=[text], model=model)\n",
    "    return response.data[0].embedding\n",
    "\n",
    "# Compute embeddings for all rows\n",
    "df[\"embedding\"] = df[\"content\"].apply(lambda x: get_embedding(x))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f70af86f",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'embedding'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyError\u001b[39m                                  Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32m/workspaces/taqathon_rag/.venv/lib/python3.12/site-packages/pandas/core/indexes/base.py:3812\u001b[39m, in \u001b[36mIndex.get_loc\u001b[39m\u001b[34m(self, key)\u001b[39m\n\u001b[32m   3811\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m3812\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_engine\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcasted_key\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   3813\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mpandas/_libs/index.pyx:167\u001b[39m, in \u001b[36mpandas._libs.index.IndexEngine.get_loc\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mpandas/_libs/index.pyx:196\u001b[39m, in \u001b[36mpandas._libs.index.IndexEngine.get_loc\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mpandas/_libs/hashtable_class_helper.pxi:7088\u001b[39m, in \u001b[36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mpandas/_libs/hashtable_class_helper.pxi:7096\u001b[39m, in \u001b[36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[31mKeyError\u001b[39m: 'embedding'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[31mKeyError\u001b[39m                                  Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[10]\u001b[39m\u001b[32m, line 4\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mfaiss\u001b[39;00m\n\u001b[32m      3\u001b[39m \u001b[38;5;66;03m# Convert to numpy array\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m4\u001b[39m embedding_matrix = np.array(\u001b[43mdf\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43membedding\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m.tolist()).astype(\u001b[33m\"\u001b[39m\u001b[33mfloat32\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m      6\u001b[39m \u001b[38;5;66;03m# Build FAISS index\u001b[39;00m\n\u001b[32m      7\u001b[39m index = faiss.IndexFlatL2(embedding_matrix.shape[\u001b[32m1\u001b[39m])\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/workspaces/taqathon_rag/.venv/lib/python3.12/site-packages/pandas/core/frame.py:4107\u001b[39m, in \u001b[36mDataFrame.__getitem__\u001b[39m\u001b[34m(self, key)\u001b[39m\n\u001b[32m   4105\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.columns.nlevels > \u001b[32m1\u001b[39m:\n\u001b[32m   4106\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._getitem_multilevel(key)\n\u001b[32m-> \u001b[39m\u001b[32m4107\u001b[39m indexer = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mcolumns\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   4108\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m is_integer(indexer):\n\u001b[32m   4109\u001b[39m     indexer = [indexer]\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/workspaces/taqathon_rag/.venv/lib/python3.12/site-packages/pandas/core/indexes/base.py:3819\u001b[39m, in \u001b[36mIndex.get_loc\u001b[39m\u001b[34m(self, key)\u001b[39m\n\u001b[32m   3814\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(casted_key, \u001b[38;5;28mslice\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m (\n\u001b[32m   3815\u001b[39m         \u001b[38;5;28misinstance\u001b[39m(casted_key, abc.Iterable)\n\u001b[32m   3816\u001b[39m         \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28many\u001b[39m(\u001b[38;5;28misinstance\u001b[39m(x, \u001b[38;5;28mslice\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m casted_key)\n\u001b[32m   3817\u001b[39m     ):\n\u001b[32m   3818\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m InvalidIndexError(key)\n\u001b[32m-> \u001b[39m\u001b[32m3819\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(key) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01merr\u001b[39;00m\n\u001b[32m   3820\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[32m   3821\u001b[39m     \u001b[38;5;66;03m# If we have a listlike key, _check_indexing_error will raise\u001b[39;00m\n\u001b[32m   3822\u001b[39m     \u001b[38;5;66;03m#  InvalidIndexError. Otherwise we fall through and re-raise\u001b[39;00m\n\u001b[32m   3823\u001b[39m     \u001b[38;5;66;03m#  the TypeError.\u001b[39;00m\n\u001b[32m   3824\u001b[39m     \u001b[38;5;28mself\u001b[39m._check_indexing_error(key)\n",
      "\u001b[31mKeyError\u001b[39m: 'embedding'"
     ]
    }
   ],
   "source": [
    "import faiss\n",
    "\n",
    "# Convert to numpy array\n",
    "embedding_matrix = np.array(df[\"embedding\"].tolist()).astype(\"float32\")\n",
    "\n",
    "# Build FAISS index\n",
    "index = faiss.IndexFlatL2(embedding_matrix.shape[1])\n",
    "index.add(embedding_matrix)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4dd0e0a4",
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'OpenAI' from 'openai' (/workspaces/taqathon_rag/.venv/lib/python3.12/site-packages/openai/__init__.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mImportError\u001b[39m                               Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[11]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# def ask_rag(query, ):\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mopenai\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m OpenAI\n\u001b[32m      4\u001b[39m client = OpenAI(api_key=\u001b[33m\"\u001b[39m\u001b[33msk-proj-0aqj_iN7CUI0ezyc7Lreswv0_X0cIf3iiargDc1Yw91fbW3mR2TRPbotAxR4siVsV0hXXYzERtT3BlbkFJ92QzlyaIgMfLael1mWyuvHLaciOewr256e1nnvmllUV_oIW1NlbhjDxgG_4daCWjR1nRkpEnoA\u001b[39m\u001b[33m\"\u001b[39m)  \u001b[38;5;66;03m# your project key here\u001b[39;00m\n\u001b[32m      8\u001b[39m query = \u001b[33m\"\u001b[39m\u001b[33mdonner moi tout les anomalie a une relation avec transformateur princiale par order?, trier par order en file\u001b[39m\u001b[33m\"\u001b[39m\n",
      "\u001b[31mImportError\u001b[39m: cannot import name 'OpenAI' from 'openai' (/workspaces/taqathon_rag/.venv/lib/python3.12/site-packages/openai/__init__.py)"
     ]
    }
   ],
   "source": [
    "# def ask_rag(query, ):\n",
    "from openai import OpenAI\n",
    "\n",
    "client = OpenAI(api_key=\"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\")  # your project key here\n",
    "\n",
    "\n",
    "\n",
    "query = \"donner moi tout les anomalie a une relation avec transformateur princiale par order?, trier par order en file\"\n",
    "\n",
    "top_k=5\n",
    "query_embedding = np.array(get_embedding(query)).astype(\"float32\").reshape(1, -1)\n",
    "D, I = index.search(query_embedding, top_k)\n",
    "contexts = df.iloc[I[0]][\"content\"].tolist()\n",
    "\n",
    "# Concatenate retrieved context\n",
    "prompt = \"Voici des anomalies liées aux équipements:\\n\"\n",
    "for i, ctx in enumerate(contexts):\n",
    "    prompt += f\"{i+1}. {ctx}\\n\"\n",
    "prompt += f\"\\nÀ partir de ces informations, réponds à la question suivante:\\n{query}\"\n",
    "response = client.chat.completions.create(\n",
    "    model=\"gpt-4-turbo\",\n",
    "    messages=[\n",
    "        {\"role\": \"user\", \"content\": prompt}\n",
    "    ]\n",
    ")\n",
    "\n",
    "    # return response.choices[0].message.content\n",
    "print(response.choices[0].message.content)\n",
    "\n",
    "# Example\n",
    "# print(ask_rag(\"Quels sont les anomalies qui peuvent causer une perte de sécurité de process ?\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fc24915",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load new anomaly file\n",
    "new_df = pd.read_excel(\"test01wfi.xlsx\")\n",
    "\n",
    "# Combine into a \"content\" column just like during training\n",
    "new_df[\"content\"] = (\n",
    "    new_df[\"Systeme\"].astype(str) + \" | \" +\n",
    "    new_df[\"Description\"].astype(str) + \" | \" +\n",
    "    new_df[\"Description de l'équipement\"].astype(str)\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1c9e35e",
   "metadata": {},
   "outputs": [
    {
     "ename": "NotFittedError",
     "evalue": "Pipeline is not fitted yet.",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNotFittedError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32m/workspaces/taqathon_rag/.venv/lib/python3.12/site-packages/sklearn/pipeline.py:53\u001b[39m, in \u001b[36m_raise_or_warn_if_not_fitted\u001b[39m\u001b[34m(estimator)\u001b[39m\n\u001b[32m     52\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m53\u001b[39m     \u001b[38;5;28;01myield\u001b[39;00m\n\u001b[32m     54\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m NotFittedError \u001b[38;5;28;01mas\u001b[39;00m exc:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/workspaces/taqathon_rag/.venv/lib/python3.12/site-packages/sklearn/pipeline.py:786\u001b[39m, in \u001b[36mPipeline.predict\u001b[39m\u001b[34m(self, X, **params)\u001b[39m\n\u001b[32m    785\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m _, name, transform \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m._iter(with_final=\u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[32m--> \u001b[39m\u001b[32m786\u001b[39m     Xt = \u001b[43mtransform\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtransform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mXt\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    787\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m.steps[-\u001b[32m1\u001b[39m][\u001b[32m1\u001b[39m].predict(Xt, **params)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/workspaces/taqathon_rag/.venv/lib/python3.12/site-packages/sklearn/feature_extraction/text.py:2126\u001b[39m, in \u001b[36mTfidfVectorizer.transform\u001b[39m\u001b[34m(self, raw_documents)\u001b[39m\n\u001b[32m   2111\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"Transform documents to document-term matrix.\u001b[39;00m\n\u001b[32m   2112\u001b[39m \n\u001b[32m   2113\u001b[39m \u001b[33;03mUses the vocabulary and document frequencies (df) learned by fit (or\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m   2124\u001b[39m \u001b[33;03m    Tf-idf-weighted document-term matrix.\u001b[39;00m\n\u001b[32m   2125\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m2126\u001b[39m \u001b[43mcheck_is_fitted\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmsg\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mThe TF-IDF vectorizer is not fitted\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m   2128\u001b[39m X = \u001b[38;5;28msuper\u001b[39m().transform(raw_documents)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/workspaces/taqathon_rag/.venv/lib/python3.12/site-packages/sklearn/utils/validation.py:1754\u001b[39m, in \u001b[36mcheck_is_fitted\u001b[39m\u001b[34m(estimator, attributes, msg, all_or_any)\u001b[39m\n\u001b[32m   1753\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m _is_fitted(estimator, attributes, all_or_any):\n\u001b[32m-> \u001b[39m\u001b[32m1754\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m NotFittedError(msg % {\u001b[33m\"\u001b[39m\u001b[33mname\u001b[39m\u001b[33m\"\u001b[39m: \u001b[38;5;28mtype\u001b[39m(estimator).\u001b[34m__name__\u001b[39m})\n",
      "\u001b[31mNotFittedError\u001b[39m: The TF-IDF vectorizer is not fitted",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[31mNotFittedError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[6]\u001b[39m\u001b[32m, line 3\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# Use the same TF-IDF and model from earlier\u001b[39;00m\n\u001b[32m      2\u001b[39m X_new = new_df[\u001b[33m\"\u001b[39m\u001b[33mcontent\u001b[39m\u001b[33m\"\u001b[39m]\n\u001b[32m----> \u001b[39m\u001b[32m3\u001b[39m preds = \u001b[43mfast_pipeline\u001b[49m\u001b[43m.\u001b[49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_new\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      5\u001b[39m \u001b[38;5;66;03m# Round and clip to 1–5\u001b[39;00m\n\u001b[32m      6\u001b[39m preds_df = pd.DataFrame(preds, columns=[\u001b[33m\"\u001b[39m\u001b[33mFiabilité Intégrité\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mDisponibilté\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mProcess Safety\u001b[39m\u001b[33m\"\u001b[39m])\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/workspaces/taqathon_rag/.venv/lib/python3.12/site-packages/sklearn/pipeline.py:781\u001b[39m, in \u001b[36mPipeline.predict\u001b[39m\u001b[34m(self, X, **params)\u001b[39m\n\u001b[32m    739\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"Transform the data, and apply `predict` with the final estimator.\u001b[39;00m\n\u001b[32m    740\u001b[39m \n\u001b[32m    741\u001b[39m \u001b[33;03mCall `transform` of each transformer in the pipeline. The transformed\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    778\u001b[39m \u001b[33;03m    Result of calling `predict` on the final estimator.\u001b[39;00m\n\u001b[32m    779\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    780\u001b[39m \u001b[38;5;66;03m# TODO(1.8): Remove the context manager and use check_is_fitted(self)\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m781\u001b[39m \u001b[43m\u001b[49m\u001b[38;5;28;43;01mwith\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m_raise_or_warn_if_not_fitted\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m    782\u001b[39m \u001b[43m    \u001b[49m\u001b[43mXt\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mX\u001b[49m\n\u001b[32m    784\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m_routing_enabled\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/python/current/lib/python3.12/contextlib.py:158\u001b[39m, in \u001b[36m_GeneratorContextManager.__exit__\u001b[39m\u001b[34m(self, typ, value, traceback)\u001b[39m\n\u001b[32m    156\u001b[39m     value = typ()\n\u001b[32m    157\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m158\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mgen\u001b[49m\u001b[43m.\u001b[49m\u001b[43mthrow\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    159\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[32m    160\u001b[39m     \u001b[38;5;66;03m# Suppress StopIteration *unless* it's the same exception that\u001b[39;00m\n\u001b[32m    161\u001b[39m     \u001b[38;5;66;03m# was passed to throw().  This prevents a StopIteration\u001b[39;00m\n\u001b[32m    162\u001b[39m     \u001b[38;5;66;03m# raised inside the \"with\" statement from being suppressed.\u001b[39;00m\n\u001b[32m    163\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m exc \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m value\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/workspaces/taqathon_rag/.venv/lib/python3.12/site-packages/sklearn/pipeline.py:55\u001b[39m, in \u001b[36m_raise_or_warn_if_not_fitted\u001b[39m\u001b[34m(estimator)\u001b[39m\n\u001b[32m     53\u001b[39m     \u001b[38;5;28;01myield\u001b[39;00m\n\u001b[32m     54\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m NotFittedError \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[32m---> \u001b[39m\u001b[32m55\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m NotFittedError(\u001b[33m\"\u001b[39m\u001b[33mPipeline is not fitted yet.\u001b[39m\u001b[33m\"\u001b[39m) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mexc\u001b[39;00m\n\u001b[32m     57\u001b[39m \u001b[38;5;66;03m# we only get here if the above didn't raise\u001b[39;00m\n\u001b[32m     58\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "\u001b[31mNotFittedError\u001b[39m: Pipeline is not fitted yet."
     ]
    }
   ],
   "source": [
    "# Use the same TF-IDF and model from earlier\n",
    "X_new = new_df[\"content\"]\n",
    "preds = fast_pipeline.predict(X_new)\n",
    "\n",
    "# Round and clip to 1–5\n",
    "preds_df = pd.DataFrame(preds, columns=[\"Fiabilité Intégrité\", \"Disponibilté\", \"Process Safety\"])\n",
    "preds_df = preds_df.round().clip(1, 5).astype(int)\n",
    "\n",
    "# Merge with input\n",
    "scored_df = pd.concat([new_df, preds_df], axis=1)\n",
    "\n",
    "# Export or display\n",
    "scored_df.to_excel(\"scored_anomalies.xlsx\", index=False)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
